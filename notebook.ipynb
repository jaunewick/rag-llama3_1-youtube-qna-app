{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"llama3.1\"\n",
    "\n",
    "# Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI\n",
    "YOUTUBE_VIDEO_URL = \"https://www.youtube.com/watch?v=L_Guz73e6fw\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up llm/slm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(api_key=OPENAI_API_KEY,model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple model test to verify if it's working.\n",
    "\n",
    "Extracting the response by chaining the model with an output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\nWhat do you call a fake noodle?\\n\\nAn impasta!\\n\\nHope that made you laugh! Do you want to hear another one?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "chain.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based on the context below.\n",
      "If you can't answer the question, reply \"I don't know\"\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below.\n",
    "If you can't answer the question, reply \"I don't know\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple test to see if the model & prompt template was done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniel.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"The name I was given was Daniel\",\n",
    "    \"question\": \"What's my name ?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate translation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Il a commandé 1 plat (calamari). \\n\\nOr, cela traduit mieux :\\n\\n\" Il a commandé une portion de calamars\".'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate {answer} to {language}\"\n",
    ")\n",
    "\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
    ")\n",
    "\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Daniel want to order calamari for lunch. He does'nt like anything else.\",\n",
    "        \"question\": \"How many dishes did Daniel ordered?\",\n",
    "        \"language\": \"français\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing YouTube Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcribing a ~2 and half hour video, the following code might take a few minutes.\n",
    "\n",
    "Note : Using pytubefix instead of pytube. The developers of this library are trying to fix the HTTP 400 response error as of August 21st 2024..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:13<00:00, 11.1MiB/s]\n",
      "/Users/danielgiaoo/Developer/GitHub/rag-youtube-qna-app/.venv/lib/python3.12/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pytubefix import YouTube\n",
    "import whisper\n",
    "\n",
    "\n",
    "if not os.path.exists(\"ytb_transcript.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO_URL)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "    # \"base\" model used for ok accuracy and fast transcribing\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "        with open(\"ytb_transcript.txt\", \"w\") as file:\n",
    "            file.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the transcript into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ytb_transcript.txt'}, page_content=\"We have been a misunderstood and badly mocked ork for a long time. Like when we started, we announced the work at the end of 2015. And said we were going to work on AGI. Like people thought we were batshit and sane. Yeah. You know, like I remember at the time, a eminent AI scientist at a large industrial AI lab was like DMing individual reporters being like, you know, these people are very good and it's ridiculous to talk about AGI. And I can't believe you're giving them time of day. And it's like, that was the level of like, pettiness and rancor in the field that a new group of people saying we're going to try to build AGI. So open AI and deep mind was a small collection of folks who were brave enough to talk about AGI in the face of mockery. We don't get mocked as much now. Don't get mocked as much now. The following is a conversation with Sam Altman, CEO of OpenAI, the company behind GPT4, JADGPT, Dolly, Codex, and many other AI technologies which both individually and together\"),\n",
       " Document(metadata={'source': 'ytb_transcript.txt'}, page_content=\"and together constitute some of the greatest breakthroughs in the history of artificial intelligence, computing and humanity in general. Please allow me to say a few words about the possibilities and the dangers of AI in this current moment in the history of human civilization. I believe it is a critical moment. We stand on the precipice of fundamental societal transformation, where soon nobody knows when, but many including me believe it's within our lifetime. The collective intelligence of the human species begins to pale in comparison by many orders of magnitude to the general superintelligence in the AI systems we build and deploy. At scale, this is both exciting and terrifying. It is exciting because of the innumerable applications we know and don't yet know that will empower humans to create, to flourish, to escape the widespread poverty and suffering that exists in the world today and to succeed in that old, all-too-human pursuit of happiness. It is terrifying because of the\"),\n",
       " Document(metadata={'source': 'ytb_transcript.txt'}, page_content=\"because of the power that superintelligent AI wields the destroy human civilization intentionally or unintentionally. The power to suffocate the human spirit in the totalitarian way of George Orwell's 1984 or the Pleasure-Fueled Mass hysteria of Brave New World, where as Huxley saw it, people come to love their oppression, to adore the technologies that undo their capacities to think. That is why these conversations with the leaders, engineers, and philosophers both optimists and cynics is important now. These are not merely technical conversations about AI. These are conversations about power, about companies, institutions, and political systems that deploy, check, and balance this power, about distributed economic systems that incentivize the safety and human alignment of this power, about the psychology of the engineers and leaders that deploy AI, and about the history of human nature, our capacity for good and evil at scale. I'm deeply honored to have gotten to know and to have\"),\n",
       " Document(metadata={'source': 'ytb_transcript.txt'}, page_content=\"to know and to have spoken with on and off the mic with many folks who now work at OpenAI, including Sam Altman, Greg Brockman, Iliasetskever, Wojtek Zaremba, Andrzej Karpathy, Jakob Pachaki, and many others. It means the world that Sam has been totally open with me, willing to have multiple conversations, including challenging ones, on and off the mic. I will continue to have these conversations, to both celebrate the incredible accomplishments of the AI community, and to steel man the critical perspective on major decisions, various companies, and leaders make, always with the goal of trying to help in my small way. If I fail, I will work hard to improve. I love you all. This is Alex Viennit podcast. To support it, please check out our sponsors in the description, and now dear friends, here Sam Altman. High level, what is GPT for? How does it work, and what to use most amazing about it? It's a system that we'll look back at and say it was a very early AI, and it will, it's slow,\"),\n",
       " Document(metadata={'source': 'ytb_transcript.txt'}, page_content=\"it will, it's slow, it's buggy, it doesn't do a lot of things very well, but neither did the very earliest computers, and they still pointed a path to something that was going to be really important in our lives, even though it took a few decades to evolve. Do you think this is a pivotal moment? Like out of all the versions of GPT, 50 years from now, when they look back on an early system, that was really kind of a leap, you know, in a Wikipedia page about the history of artificial intelligence, which of the GPT's would they put? That is a good question. I sort of think of progress as this continual exponential. It's not like we could say here was the moment where AI went from, not happening to happening, and I'd have a very hard time like pinpointing a single thing. I think it's this very continual curve. Will the history books write about GPT one or two or three or four or seven? That's for them to decide. I don't really know. I think if I had to pick some moment from what we've\")]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"ytb_transcript.txt\")\n",
    "txt_documents = loader.load()\n",
    "txt_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "txt_splitter.split_documents(txt_documents)[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
